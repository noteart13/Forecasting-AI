"""
Anomaly Detection Models for ICS/SCADA Networks
Ph√°t hi·ªán: Lateral Movement, Data Exfiltration, ICS Anomalies
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import joblib
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ICSAnomalyDetector:
    """
    Ph√°t hi·ªán b·∫•t th∆∞·ªùng trong m·∫°ng ICS/SCADA
    S·ª≠ d·ª•ng Isolation Forest v√† rule-based detection
    """
    
    def __init__(self, contamination=0.05):
        self.contamination = contamination
        self.model = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100
        )
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.is_trained = False
        
    def train(self, df: pd.DataFrame, feature_cols: list = None):
        """
        Train model tr√™n normal traffic
        
        Args:
            df: DataFrame ch·ª©a normal network traffic
            feature_cols: List t√™n c·ªôt ƒë·ªÉ d√πng l√†m features
        """
        if feature_cols is None:
            # Auto-select numeric columns
            feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            # Remove IDs and labels
            exclude = ['src_ip', 'dst_ip', 'timestamp', 'label', 'anomaly']
            feature_cols = [c for c in feature_cols if c not in exclude]
        
        self.feature_columns = feature_cols
        logger.info(f"Training with {len(feature_cols)} features: {feature_cols}")
        
        X = df[feature_cols].fillna(0)
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        # Train Isolation Forest
        self.model.fit(X_scaled)
        self.is_trained = True
        
        logger.info(f"‚úÖ Model trained on {len(df)} normal flows")
        
    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Predict anomalies
        
        Returns:
            DataFrame v·ªõi c·ªôt 'anomaly_score' v√† 'is_anomaly'
        """
        if not self.is_trained:
            raise ValueError("Model not trained")
        
        X = df[self.feature_columns].fillna(0)
        X_scaled = self.scaler.transform(X)
        
        # Predict (-1 = anomaly, 1 = normal)
        predictions = self.model.predict(X_scaled)
        scores = self.model.score_samples(X_scaled)
        
        result = df.copy()
        result['anomaly_score'] = scores
        result['is_anomaly'] = (predictions == -1).astype(int)
        
        n_anomalies = result['is_anomaly'].sum()
        logger.info(f"üîç Found {n_anomalies} anomalies ({n_anomalies/len(df)*100:.2f}%)")
        
        return result
    
    def detect_lateral_movement(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ph√°t hi·ªán Lateral Movement - K·∫ª t·∫•n c√¥ng di chuy·ªÉn ngang trong h·ªá th·ªëng
        
        D·∫•u hi·ªáu n√¢ng cao:
        - Source IP k·∫øt n·ªëi v·ªõi nhi·ªÅu destinations trong th·ªùi gian ng·∫Øn
        - Scan nhi·ªÅu ports kh√°c nhau (port scanning)
        - Connections ƒë·∫øn c√°c hosts kh√¥ng th√¥ng th∆∞·ªùng
        - Failed connections cao (connection attempts)
        - Traffic patterns b·∫•t th∆∞·ªùng (burst traffic)
        - Access t·ª´ c√°c IP ranges kh√¥ng mong ƒë·ª£i
        """
        df = df.copy()
        
        # Time window (5 ph√∫t)
        if 'timestamp' in df.columns:
            df['time_window'] = df['timestamp'].dt.floor('5T')
        else:
            df['time_window'] = 0
        
        # Group by source IP v√† time window
        lateral_stats = df.groupby(['src_ip', 'time_window']).agg({
            'dst_ip': 'nunique',
            'dst_port': 'nunique',
            'packets': 'sum',
            'bytes': 'sum',
            'duration': 'mean'
        }).reset_index()
        
        lateral_stats.columns = ['src_ip', 'time_window', 'unique_dsts', 'unique_ports', 'packets', 'bytes', 'avg_duration']
        
        # T√≠nh to√°n c√°c metrics n√¢ng cao
        lateral_stats['dst_diversity_score'] = lateral_stats['unique_dsts'] / lateral_stats['unique_dsts'].max()
        lateral_stats['port_scan_score'] = lateral_stats['unique_ports'] / lateral_stats['unique_ports'].max()
        lateral_stats['traffic_intensity'] = lateral_stats['bytes'] / (lateral_stats['avg_duration'] + 1)
        
        # Detect port scanning patterns
        lateral_stats['is_port_scan'] = (
            (lateral_stats['unique_ports'] > 10) &  # Scan nhi·ªÅu ports
            (lateral_stats['packets'] > 50)          # Nhi·ªÅu packets
        ).astype(int)
        
        # Detect host scanning patterns  
        lateral_stats['is_host_scan'] = (
            (lateral_stats['unique_dsts'] > 5) &     # Scan nhi·ªÅu hosts
            (lateral_stats['packets'] > 20)          # Nhi·ªÅu packets
        ).astype(int)
        
        # Lateral movement score (weighted combination)
        lateral_stats['lateral_score'] = (
            lateral_stats['dst_diversity_score'] * 4.0 +      # Nhi·ªÅu destinations
            lateral_stats['port_scan_score'] * 3.0 +          # Scan ports
            lateral_stats['is_port_scan'] * 2.0 +             # Port scan pattern
            lateral_stats['is_host_scan'] * 2.0              # Host scan pattern
        )
        
        # Dynamic threshold based on traffic patterns
        mean_score = lateral_stats['lateral_score'].mean()
        std_score = lateral_stats['lateral_score'].std()
        threshold = mean_score + 2 * std_score  # 2-sigma threshold
        
        lateral_stats['is_lateral_movement'] = (lateral_stats['lateral_score'] > threshold).astype(int)
        
        # Additional checks for high-risk patterns
        high_risk = lateral_stats[
            (lateral_stats['is_lateral_movement'] == 1) &
            ((lateral_stats['is_port_scan'] == 1) | (lateral_stats['is_host_scan'] == 1))
        ]
        lateral_stats['is_high_risk'] = lateral_stats.index.isin(high_risk.index).astype(int)
        
        suspicious = lateral_stats[lateral_stats['is_lateral_movement'] == 1]
        high_risk_count = lateral_stats['is_high_risk'].sum()
        
        logger.info(f"üö® Lateral Movement: {len(suspicious)} suspicious activities detected")
        logger.info(f"üö® High Risk Patterns: {high_risk_count} detected (port/host scanning)")
        
        return lateral_stats
    
    def detect_data_exfiltration(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ph√°t hi·ªán Data Exfiltration - H√†nh vi ƒë√°nh c·∫Øp v√† chuy·ªÉn d·ªØ li·ªáu ra b√™n ngo√†i
        
        D·∫•u hi·ªáu n√¢ng cao:
        - Upload traffic b·∫•t th∆∞·ªùng l·ªõn (outbound bytes)
        - Connections ra external IPs kh√¥ng th√¥ng th∆∞·ªùng
        - Transfers v√†o gi·ªù b·∫•t th∆∞·ªùng (ƒë√™m, cu·ªëi tu·∫ßn)
        - S·ª≠ d·ª•ng protocols kh√¥ng th√¥ng th∆∞·ªùng (FTP, HTTP POST, etc.)
        - Traffic patterns b·∫•t th∆∞·ªùng (burst uploads)
        - Connections ƒë·∫øn c√°c domains/IPs ƒë√°ng ng·ªù
        - S·ª≠ d·ª•ng encrypted channels ƒë·ªÉ che gi·∫•u
        """
        df = df.copy()
        
        # Identify internal/external IPs
        def is_private_ip(ip):
            """Check if IP is private (RFC 1918)"""
            if isinstance(ip, str):
                parts = ip.split('.')
                if len(parts) == 4:
                    try:
                        first = int(parts[0])
                        second = int(parts[1])
                        if first == 10:
                            return True
                        if first == 172 and 16 <= second <= 31:
                            return True
                        if first == 192 and second == 168:
                            return True
                        # Loopback
                        if first == 127:
                            return True
                    except:
                        pass
            return False
        
        df['is_internal_src'] = df['src_ip'].apply(is_private_ip)
        df['is_internal_dst'] = df['dst_ip'].apply(is_private_ip)
        
        # Outbound traffic (internal ‚Üí external)
        outbound = df[(df['is_internal_src']) & (~df['is_internal_dst'])].copy()
        
        if len(outbound) == 0:
            logger.info("No outbound traffic detected")
            return pd.DataFrame()
        
        # Time window (10 ph√∫t)
        if 'timestamp' in outbound.columns:
            outbound['time_window'] = outbound['timestamp'].dt.floor('10T')
        else:
            outbound['time_window'] = 0
        
        # Aggregate by source v√† time window
        exfil_stats = outbound.groupby(['src_ip', 'time_window']).agg({
            'bytes': 'sum',
            'packets': 'sum',
            'dst_ip': 'nunique',
            'dst_port': 'nunique',
            'duration': 'sum'
        }).reset_index()
        
        exfil_stats.columns = ['src_ip', 'time_window', 'bytes_out', 'packets_out', 'unique_external_dsts', 'unique_ports', 'total_duration']
        
        # T√≠nh to√°n c√°c metrics n√¢ng cao
        exfil_stats['mb_transferred'] = exfil_stats['bytes_out'] / (1024 * 1024)
        exfil_stats['transfer_rate'] = exfil_stats['bytes_out'] / (exfil_stats['total_duration'] + 1)
        exfil_stats['packet_rate'] = exfil_stats['packets_out'] / (exfil_stats['total_duration'] + 1)
        
        # Detect suspicious patterns
        exfil_stats['is_large_transfer'] = (exfil_stats['mb_transferred'] > 10).astype(int)  # > 10MB
        exfil_stats['is_multiple_destinations'] = (exfil_stats['unique_external_dsts'] > 3).astype(int)
        exfil_stats['is_high_bandwidth'] = (exfil_stats['transfer_rate'] > 1000000).astype(int)  # > 1MB/s
        
        # Detect unusual timing (night/weekend transfers)
        if 'timestamp' in outbound.columns:
            outbound['hour'] = outbound['timestamp'].dt.hour
            outbound['day_of_week'] = outbound['timestamp'].dt.dayofweek
            outbound['is_night'] = ((outbound['hour'] >= 22) | (outbound['hour'] <= 6)).astype(int)
            outbound['is_weekend'] = (outbound['day_of_week'] >= 5).astype(int)
            
            # Aggregate timing info
            timing_stats = outbound.groupby(['src_ip', 'time_window']).agg({
                'is_night': 'max',
                'is_weekend': 'max'
            }).reset_index()
            
            exfil_stats = exfil_stats.merge(timing_stats, on=['src_ip', 'time_window'], how='left')
            exfil_stats['is_unusual_time'] = ((exfil_stats['is_night'] == 1) | (exfil_stats['is_weekend'] == 1)).astype(int)
        else:
            exfil_stats['is_unusual_time'] = 0
        
        # Detect suspicious protocols (FTP, HTTP POST, etc.)
        suspicious_protocols = ['FTP', 'HTTP', 'HTTPS', 'SFTP', 'SCP']
        protocol_stats = outbound[outbound['protocol'].isin(suspicious_protocols)].groupby(['src_ip', 'time_window']).size().reset_index(name='suspicious_protocol_count')
        exfil_stats = exfil_stats.merge(protocol_stats, on=['src_ip', 'time_window'], how='left')
        exfil_stats['suspicious_protocol_count'] = exfil_stats['suspicious_protocol_count'].fillna(0)
        exfil_stats['is_suspicious_protocol'] = (exfil_stats['suspicious_protocol_count'] > 0).astype(int)
        
        # Exfiltration score (weighted combination)
        exfil_stats['exfil_score'] = (
            exfil_stats['mb_transferred'] * 3.0 +                    # Data volume
            exfil_stats['unique_external_dsts'] * 2.0 +              # Multiple destinations
            exfil_stats['is_large_transfer'] * 4.0 +                # Large transfers
            exfil_stats['is_multiple_destinations'] * 3.0 +          # Multiple destinations
            exfil_stats['is_high_bandwidth'] * 2.0 +                # High bandwidth
            exfil_stats['is_unusual_time'] * 2.0 +                   # Unusual timing
            exfil_stats['is_suspicious_protocol'] * 1.5              # Suspicious protocols
        )
        
        # Dynamic threshold based on traffic patterns
        mean_score = exfil_stats['exfil_score'].mean()
        std_score = exfil_stats['exfil_score'].std()
        threshold = mean_score + 2 * std_score  # 2-sigma threshold
        
        exfil_stats['is_exfiltration'] = (exfil_stats['exfil_score'] > threshold).astype(int)
        
        # Additional checks for high-risk patterns
        high_risk = exfil_stats[
            (exfil_stats['is_exfiltration'] == 1) &
            ((exfil_stats['is_large_transfer'] == 1) | (exfil_stats['is_unusual_time'] == 1))
        ]
        exfil_stats['is_high_risk'] = exfil_stats.index.isin(high_risk.index).astype(int)
        
        suspicious = exfil_stats[exfil_stats['is_exfiltration'] == 1]
        high_risk_count = exfil_stats['is_high_risk'].sum()
        
        logger.info(f"üö® Data Exfiltration: {len(suspicious)} suspicious transfers detected")
        logger.info(f"üö® High Risk Patterns: {high_risk_count} detected (large transfers/unusual timing)")
        
        return exfil_stats
    
    def detect_ics_anomalies(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ph√°t hi·ªán ICS-specific anomalies - C√°c h√†nh vi b·∫•t th∆∞·ªùng trong m·∫°ng H·ªá th·ªëng ƒêi·ªÅu khi·ªÉn C√¥ng nghi·ªáp
        
        D·∫•u hi·ªáu n√¢ng cao:
        - ICS devices k·∫øt n·ªëi v·ªõi unexpected IPs (external connections)
        - ICS protocols s·ª≠ d·ª•ng v√†o gi·ªù b·∫•t th∆∞·ªùng (night/weekend)
        - Sudden changes trong ICS traffic patterns (volume spikes)
        - Non-ICS devices accessing ICS ports (unauthorized access)
        - ICS devices communicating with non-ICS protocols
        - Abnormal ICS protocol usage patterns
        - ICS devices showing lateral movement behavior
        - Unusual ICS device discovery patterns
        """
        df = df.copy()
        
        if 'is_ics_protocol' not in df.columns:
            logger.warning("No ICS protocol information available")
            return pd.DataFrame()
        
        # Filter ICS traffic
        ics_traffic = df[df['is_ics_protocol'] == 1].copy()
        
        if len(ics_traffic) == 0:
            logger.info("No ICS protocol traffic detected")
            return pd.DataFrame()
        
        logger.info(f"üîç Analyzing {len(ics_traffic)} ICS flows...")
        
        # Initialize anomaly tracking
        anomaly_details = []
        
        # 1. ICS traffic at unusual times (night/weekend)
        if 'is_night' in ics_traffic.columns:
            night_ics = ics_traffic[ics_traffic['is_night'] == 1]
            if len(night_ics) > 0:
                anomaly_details.append({
                    'type': 'unusual_time',
                    'severity': 'medium',
                    'count': len(night_ics),
                    'description': f'{len(night_ics)} ICS connections during night hours',
                    'affected_devices': night_ics['src_ip'].nunique()
                })
        
        if 'is_weekend' in ics_traffic.columns:
            weekend_ics = ics_traffic[ics_traffic['is_weekend'] == 1]
            if len(weekend_ics) > 0:
                anomaly_details.append({
                    'type': 'weekend_activity',
                    'severity': 'medium',
                    'count': len(weekend_ics),
                    'description': f'{len(weekend_ics)} ICS connections during weekend',
                    'affected_devices': weekend_ics['src_ip'].nunique()
                })
        
        # 2. ICS devices with high destination diversity (potential lateral movement)
        ics_diversity = ics_traffic.groupby('src_ip').agg({
            'dst_ip': 'nunique',
            'dst_port': 'nunique',
            'bytes': 'sum'
        }).reset_index()
        
        ics_diversity.columns = ['src_ip', 'unique_dsts', 'unique_ports', 'total_bytes']
        
        # Calculate diversity scores
        mean_diversity = ics_diversity['unique_dsts'].mean()
        std_diversity = ics_diversity['unique_dsts'].std()
        high_diversity_threshold = mean_diversity + 2 * std_diversity
        
        high_diversity_devices = ics_diversity[ics_diversity['unique_dsts'] > high_diversity_threshold]
        if len(high_diversity_devices) > 0:
            anomaly_details.append({
                'type': 'high_diversity',
                'severity': 'high',
                'count': len(high_diversity_devices),
                'description': f'{len(high_diversity_devices)} ICS devices with unusually high connection diversity',
                'affected_devices': high_diversity_devices['src_ip'].tolist()
            })
        
        # 3. ICS devices communicating with external IPs
        def is_private_ip(ip):
            """Check if IP is private (RFC 1918)"""
            if isinstance(ip, str):
                parts = ip.split('.')
                if len(parts) == 4:
                    try:
                        first = int(parts[0])
                        second = int(parts[1])
                        if first == 10:
                            return True
                        if first == 172 and 16 <= second <= 31:
                            return True
                        if first == 192 and second == 168:
                            return True
                        if first == 127:
                            return True
                    except:
                        pass
            return False
        
        ics_traffic['is_internal_dst'] = ics_traffic['dst_ip'].apply(is_private_ip)
        external_ics = ics_traffic[~ics_traffic['is_internal_dst']]
        
        if len(external_ics) > 0:
            anomaly_details.append({
                'type': 'external_communication',
                'severity': 'critical',
                'count': len(external_ics),
                'description': f'{len(external_ics)} ICS connections to external IPs',
                'affected_devices': external_ics['src_ip'].nunique(),
                'external_ips': external_ics['dst_ip'].unique().tolist()[:10]  # Limit to first 10
            })
        
        # 4. Sudden traffic volume changes (DDoS-like patterns)
        if 'timestamp' in ics_traffic.columns:
            ics_traffic['time_window'] = ics_traffic['timestamp'].dt.floor('30T')
            volume_stats = ics_traffic.groupby('time_window').agg({
                'bytes': 'sum',
                'packets': 'sum',
                'src_ip': 'nunique'
            }).reset_index()
            
            volume_stats.columns = ['time_window', 'total_bytes', 'total_packets', 'unique_devices']
            
            # Detect volume spikes
            mean_volume = volume_stats['total_bytes'].mean()
            std_volume = volume_stats['total_bytes'].std()
            spike_threshold = mean_volume + 3 * std_volume
            
            volume_spikes = volume_stats[volume_stats['total_bytes'] > spike_threshold]
            if len(volume_spikes) > 0:
                anomaly_details.append({
                    'type': 'traffic_spike',
                    'severity': 'high',
                    'count': len(volume_spikes),
                    'description': f'{len(volume_spikes)} time windows with unusual ICS traffic volume',
                    'max_volume_mb': volume_spikes['total_bytes'].max() / (1024 * 1024)
                })
        
        # 5. Non-ICS devices accessing ICS ports (unauthorized access)
        non_ics_accessing_ics = df[(df['is_ics_protocol'] == 0) & (df['dst_port'].isin([102, 502, 2404, 20000, 44818, 47808, 4840]))]
        
        if len(non_ics_accessing_ics) > 0:
            anomaly_details.append({
                'type': 'unauthorized_access',
                'severity': 'critical',
                'count': len(non_ics_accessing_ics),
                'description': f'{len(non_ics_accessing_ics)} non-ICS devices accessing ICS ports',
                'affected_devices': non_ics_accessing_ics['src_ip'].nunique(),
                'accessed_ports': non_ics_accessing_ics['dst_port'].unique().tolist()
            })
        
        # 6. ICS protocol usage patterns analysis
        protocol_stats = ics_traffic.groupby('dst_port').agg({
            'src_ip': 'nunique',
            'bytes': 'sum',
            'packets': 'sum'
        }).reset_index()
        
        protocol_stats.columns = ['port', 'unique_sources', 'total_bytes', 'total_packets']
        
        # Detect unusual protocol usage
        unusual_protocols = protocol_stats[
            (protocol_stats['unique_sources'] > protocol_stats['unique_sources'].quantile(0.9)) |
            (protocol_stats['total_bytes'] > protocol_stats['total_bytes'].quantile(0.9))
        ]
        
        if len(unusual_protocols) > 0:
            anomaly_details.append({
                'type': 'unusual_protocol_usage',
                'severity': 'medium',
                'count': len(unusual_protocols),
                'description': f'{len(unusual_protocols)} ICS protocols showing unusual usage patterns',
                'protocols': unusual_protocols['port'].tolist()
            })
        
        # Convert to DataFrame
        if anomaly_details:
            result_df = pd.DataFrame(anomaly_details)
            
            # Calculate overall risk score
            severity_scores = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}
            result_df['risk_score'] = result_df['severity'].map(severity_scores)
            
            logger.info(f"üö® ICS Anomalies: {len(anomaly_details)} anomaly types detected")
            for _, row in result_df.iterrows():
                logger.info(f"   - {row['type']} ({row['severity']}): {row['description']}")
            
            return result_df
        else:
            logger.info("‚úÖ No ICS anomalies detected")
            return pd.DataFrame()
    
    def save_model(self, filepath: str):
        """Save trained model"""
        joblib.dump({
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'contamination': self.contamination
        }, filepath)
        logger.info(f"‚úÖ Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """Load trained model"""
        data = joblib.load(filepath)
        self.model = data['model']
        self.scaler = data['scaler']
        self.feature_columns = data['feature_columns']
        self.contamination = data['contamination']
        self.is_trained = True
        logger.info(f"‚úÖ Model loaded from {filepath}")
    
    def generate_attack_summary(self, results, lateral_df=None, exfil_df=None, ics_df=None, output_path='output/attack_summary.txt', pcap_file=None):
        """
        Generate professional attack summary report with analysis tables
        
        Args:
            results: DataFrame with anomaly detection results
            lateral_df: Lateral movement detection results
            exfil_df: Data exfiltration detection results
            ics_df: ICS anomaly detection results
            output_path: Path to save summary report
            pcap_file: Original PCAP file path (if any)
        """
        from datetime import datetime
        import json
        from pathlib import Path
        
        report_lines = []
        
        # === HEADER ===
        report_lines.append("="*110)
        report_lines.append("üîí B√ÅO C√ÅO PH√ÇN T√çCH B·∫¢O M·∫¨T M·∫†NG - PH√ÅT HI·ªÜN M·ªêI ƒêE D·ªåA ICS/SCADA")
        report_lines.append("="*110)
        report_lines.append(f"Th·ªùi gian ph√¢n t√≠ch: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if pcap_file:
            report_lines.append(f"File PCAP: {Path(pcap_file).name}")
        report_lines.append(f"C√¥ng c·ª•: ICS/SCADA Anomaly Detection System v1.0")
        report_lines.append(f"Model: output/anomaly_model.pkl (Isolation Forest)")
        report_lines.append("")
        
        # Calculate statistics
        total_flows = len(results)
        total_anomalies = results['is_anomaly'].sum() if 'is_anomaly' in results.columns else 0
        anomaly_rate = (total_anomalies / total_flows * 100) if total_flows > 0 else 0
        
        high_risk = medium_risk = low_risk = 0
        if 'anomaly_score' in results.columns:
            high_risk = (results['anomaly_score'] < -0.5).sum()
            medium_risk = ((results['anomaly_score'] >= -0.5) & (results['anomaly_score'] < -0.3)).sum()
            low_risk = (results['anomaly_score'] >= -0.3).sum()
        
        # ICS traffic count
        ics_flows = 0
        if 'is_ics_protocol' in results.columns:
            ics_flows = int(results['is_ics_protocol'].sum())
        
        # Count threats
        lateral_count = 0
        if lateral_df is not None and len(lateral_df) > 0 and 'is_lateral_movement' in lateral_df.columns:
            lateral_count = int(lateral_df['is_lateral_movement'].sum())
        
        exfil_count = 0
        if exfil_df is not None and len(exfil_df) > 0 and 'is_exfiltration' in exfil_df.columns:
            exfil_count = int(exfil_df['is_exfiltration'].sum())
        
        ics_anomaly_count = len(ics_df) if ics_df is not None else 0
        
        # === 1Ô∏è‚É£ T·ªîNG QUAN ===
        report_lines.append("1Ô∏è‚É£ T·ªîNG QUAN")
        report_lines.append("-" * 110)
        report_lines.append(f"Model n·∫°p t·ª´ output/anomaly_model.pkl (ƒë√£ train ·ªü b∆∞·ªõc tr∆∞·ªõc).")
        report_lines.append(f"{total_anomalies}/{total_flows} flow b·ªã ƒë√°nh d·∫•u b·∫•t th∆∞·ªùng ‚Üí {anomaly_rate:.2f}% anomaly")
        if anomaly_rate < 5:
            report_lines.append(f"‚Üí H·ª£p l√Ω v√¨ d·ªØ li·ªáu demo nh·ªè, kh√¥ng ch·ª©a pattern l·∫° theo feature model ƒë√£ h·ªçc.")
        elif anomaly_rate > 20:
            report_lines.append(f"‚Üí T·ª∑ l·ªá b·∫•t th∆∞·ªùng cao, c·∫ßn ƒëi·ªÅu tra th√™m c√°c flow nghi ng·ªù.")
        report_lines.append("")
        
        # === 2Ô∏è‚É£ D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO ===
        report_lines.append("2Ô∏è‚É£ D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO")
        report_lines.append("-" * 110)
        report_lines.append(f"  ‚Ä¢ T·ªïng s·ªë flows: {total_flows:,}")
        
        # Network statistics
        if 'src_ip' in results.columns:
            unique_src = results['src_ip'].nunique()
            report_lines.append(f"  ‚Ä¢ Unique source IPs: {unique_src}")
        if 'dst_ip' in results.columns:
            unique_dst = results['dst_ip'].nunique()
            report_lines.append(f"  ‚Ä¢ Unique destination IPs: {unique_dst}")
        if 'protocol' in results.columns:
            protocols = results['protocol'].value_counts()
            report_lines.append(f"  ‚Ä¢ Protocols: {', '.join([f'{p}={c}' for p, c in protocols.items()])}")
        if 'bytes' in results.columns:
            total_bytes = results['bytes'].sum()
            report_lines.append(f"  ‚Ä¢ Total traffic: {total_bytes:,.0f} bytes ({total_bytes/1024/1024:.2f} MB)")
        
        report_lines.append("")
        report_lines.append("K·∫øt lu·∫≠n:")
        if total_flows < 100:
            report_lines.append("  ‚Üí D·ªØ li·ªáu PCAP r·∫•t nh·ªè, ph√π h·ª£p cho demo test, kh√¥ng ph·∫£i traffic th·ª±c.")
        else:
            report_lines.append("  ‚Üí D·ªØ li·ªáu ƒë·ªß l·ªõn ƒë·ªÉ ph√¢n t√≠ch m·ªëi ƒëe d·ªça.")
        report_lines.append("")
        
        # === 3Ô∏è‚É£ PH√ÇN T√çCH B·∫∞NG M√î H√åNH ===
        report_lines.append("3Ô∏è‚É£ PH√ÇN T√çCH B·∫∞NG M√î H√åNH")
        report_lines.append("-" * 110)
        
        if 'anomaly_score' in results.columns:
            high_risk = (results['anomaly_score'] < -0.5).sum()
            medium_risk = ((results['anomaly_score'] >= -0.5) & (results['anomaly_score'] < -0.3)).sum()
            low_risk = (results['anomaly_score'] >= -0.3).sum()
            report_lines.append(f"  üî¥ High Risk (score < -0.5): {high_risk}")
            report_lines.append(f"  üü° Medium Risk (score -0.5 to -0.3): {medium_risk}")
            report_lines.append(f"  üü¢ Low Risk (score > -0.3): {low_risk}")
        
        # === 3.1 General Anomaly Detection ===
        report_lines.append("3.1 General Anomaly Detection")
        report_lines.append("")
        report_lines.append(f"üîç Running anomaly detection...")
        report_lines.append(f"Found {total_anomalies} anomalies ({anomaly_rate:.2f}%)")
        report_lines.append(f"üö® GENERAL ANOMALIES: {total_anomalies} detected")
        report_lines.append("")
        report_lines.append("‚û°Ô∏è Gi·∫£i th√≠ch:")
        if total_anomalies > 0:
            report_lines.append(f"M√¥ h√¨nh ph√°t hi·ªán {total_anomalies} flow ({anomaly_rate:.1f}%) kh√°c bi·ªát ƒë√°ng k·ªÉ so v·ªõi {total_flows - total_anomalies} flow c√≤n l·∫°i.")
            report_lines.append(f"V√¨ contamination m·∫∑c ƒë·ªãnh ~0.05‚Äì0.15 n√™n con s·ªë {anomaly_rate:.1f}% l√† h·ª£p l√Ω.")
        else:
            report_lines.append("Kh√¥ng ph√°t hi·ªán flow b·∫•t th∆∞·ªùng n√†o. T·∫•t c·∫£ traffic ƒë·ªÅu n·∫±m trong baseline.")
        report_lines.append("K·∫øt qu·∫£ chi ti·∫øt l∆∞u trong: output/pcap_analysis/general_anomalies.csv")
        report_lines.append("")
        
        # === 3.2 Ph√¢n t√≠ch chuy√™n s√¢u ===
        report_lines.append("3.2 Ph√¢n t√≠ch Chuy√™n S√¢u")
        report_lines.append("")
        
        # === Lateral Movement ===
        report_lines.append("üî∏ Lateral Movement")
        if lateral_df is not None and len(lateral_df) > 0:
            report_lines.append(f"üö® Lateral Movement: {lateral_count} suspicious activities detected")
            
            if lateral_count > 0 and 'src_ip' in lateral_df.columns:
                report_lines.append("")
                report_lines.append("TOP SUSPICIOUS SOURCES:")
                # Get detailed info
                suspicious = lateral_df[lateral_df['is_lateral_movement'] == 1]
                for _, row in suspicious.head(5).iterrows():
                    src_ip = row.get('src_ip', 'N/A')
                    # Try both column names (unique_dsts or unique_destinations)
                    unique_dsts = row.get('unique_dsts', row.get('unique_destinations', 0))
                    unique_ports = row.get('unique_ports', 0)
                    lateral_score = row.get('lateral_score', 0)
                    report_lines.append(f"{src_ip:<20} unique_dsts={unique_dsts:<3}  unique_ports={unique_ports:<3}  lateral_score={lateral_score:.1f}")
                
                report_lines.append("")
                report_lines.append("‚û°Ô∏è Ph√¢n t√≠ch:")
                if len(suspicious) > 0:
                    top = suspicious.iloc[0]
                    top_ip = top.get('src_ip', 'N/A')
                    top_dsts = top.get('unique_dsts', top.get('unique_destinations', 0))
                    report_lines.append(f"IP {top_ip} ƒë∆∞·ª£c ƒë√°nh d·∫•u ƒë√°ng ng·ªù v√¨:")
                    report_lines.append(f"  ‚Ä¢ N√≥ k·∫øt n·ªëi ƒë·∫øn {top_dsts} ƒë√≠ch kh√°c nhau")
                    report_lines.append(f"  ‚Ä¢ Trong kho·∫£ng th·ªùi gian ng·∫Øn")
                    report_lines.append(f"  ‚Ä¢ 'Lateral Score' m·ª©c trung b√¨nh‚Äìcao; cho th·∫•y host c√≥ th·ªÉ ƒëang qu√©t/di chuy·ªÉn ngang.")
        else:
            report_lines.append(f"üö® Lateral Movement: 0 suspicious activities detected")
            report_lines.append("")
            report_lines.append("‚Üí Kh√¥ng ph√°t hi·ªán h√†nh vi di chuy·ªÉn ngang trong m·∫°ng.")
        
        report_lines.append("")
        
        # === Data Exfiltration ===
        report_lines.append("üî∏ Data Exfiltration")
        if exfil_df is not None and len(exfil_df) > 0:
            report_lines.append(f"üö® Data Exfiltration: {exfil_count} suspicious transfers detected")
            
            if exfil_count > 0:
                report_lines.append("")
                if 'bytes' in exfil_df.columns:
                    total_exfil_bytes = exfil_df['bytes'].sum()
                    report_lines.append(f"Total Data at Risk: {total_exfil_bytes:,.0f} bytes ({total_exfil_bytes/1024/1024:.2f} MB)")
                if 'dst_ip' in exfil_df.columns:
                    external_ips = exfil_df['dst_ip'].value_counts().head(3)
                    report_lines.append("Top External Destinations:")
                    for ip, count in external_ips.items():
                        report_lines.append(f"  ‚Ä¢ {ip}: {count} transfers")
        else:
            report_lines.append(f"üö® Data Exfiltration: 0 suspicious transfers detected")
            report_lines.append("")
            report_lines.append("‚Üí Kh√¥ng th·∫•y flow n√†o truy·ªÅn d·ªØ li·ªáu ra ngo√†i l·ªõn b·∫•t th∆∞·ªùng.")
        
        report_lines.append("")
        
        # === ICS-specific Anomalies ===
        report_lines.append("üî∏ ICS-specific Anomalies")
        if ics_df is not None and len(ics_df) > 0:
            report_lines.append(f"üö® ICS/SCADA Anomalies: {len(ics_df)} types detected")
            report_lines.append("")
            
            if 'severity' in ics_df.columns:
                critical = (ics_df['severity'] == 'critical').sum()
                high = (ics_df['severity'] == 'high').sum()
                medium = (ics_df['severity'] == 'medium').sum()
                report_lines.append(f"  üî¥ Critical: {critical}")
                report_lines.append(f"  üü† High: {high}")
                report_lines.append(f"  üü° Medium: {medium}")
                report_lines.append("")
            
            report_lines.append("Anomaly Types:")
            for _, row in ics_df.head(10).iterrows():
                report_lines.append(f"  ‚Ä¢ {row.get('type', 'Unknown')}: {row.get('description', 'No description')}")
        else:
            if ics_flows == 0:
                report_lines.append("No ICS protocol traffic detected")
                report_lines.append("")
                report_lines.append("‚Üí PCAP kh√¥ng ch·ª©a g√≥i Modbus/DNP3/S7Comm, n√™n module ICS kh√¥ng ch·∫°y.")
            else:
                report_lines.append(f"Found {ics_flows} ICS flows but no anomalies detected.")
                report_lines.append("")
                report_lines.append("‚Üí ICS traffic trong ph·∫°m vi b√¨nh th∆∞·ªùng.")
        
        # === 4Ô∏è‚É£ ƒê√ÅNH GI√Å T·ªîNG TH·ªÇ & √ù NGHƒ®A ===
        report_lines.append("4Ô∏è‚É£ ƒê√ÅNH GI√Å T·ªîNG TH·ªÇ & √ù NGHƒ®A")
        report_lines.append("-" * 110)
        report_lines.append("")
        
        # Create evaluation table
        report_lines.append(f"{'Th√†nh ph·∫ßn':<30} | {'ƒê√°nh gi√°':<30} | {'√ù nghƒ©a th·ª±c t·∫ø'}")
        report_lines.append("-" * 110)
        
        # Row 1: Data PCAP
        data_assess = f"{total_flows} flows"
        if total_flows < 100:
            data_meaning = "Demo test, kh√¥ng ph·∫£i traffic th·ª±c."
        else:
            data_meaning = "D·ªØ li·ªáu th·ª±c t·∫ø, ƒë·ªß l·ªõn ƒë·ªÉ ph√¢n t√≠ch."
        report_lines.append(f"{'D·ªØ li·ªáu PCAP':<30} | {data_assess:<30} | {data_meaning}")
        
        # Row 2: Model training
        model_assess = "OK, ƒë√£ train baseline"
        model_meaning = "Training tr·ª±c ti·∫øp tr√™n PCAP ƒë·ªÉ ch·∫°y online detection."
        report_lines.append(f"{'Model training':<30} | {model_assess:<30} | {model_meaning}")
        
        # Row 3: Anomaly detection
        anomaly_assess = f"{total_anomalies}/{total_flows} flow b·∫•t th∆∞·ªùng"
        if total_anomalies > 0:
            anomaly_meaning = f"C√≥ ho·∫°t ƒë·ªông l·∫° c·∫ßn xem x√©t th√™m."
        else:
            anomaly_meaning = "Kh√¥ng c√≥ outlier r√µ r√†ng."
        report_lines.append(f"{'Anomaly detection':<30} | {anomaly_assess:<30} | {anomaly_meaning}")
        
        # Row 4: Lateral Movement
        lateral_assess = f"{lateral_count} ngu·ªìn nghi ng·ªù"
        if lateral_count > 0:
            lateral_meaning = "C√≥ th·ªÉ l√† thƒÉm d√≤ n·ªôi b·ªô nh·∫π ho·∫∑c qu√©t m·∫°ng."
        else:
            lateral_meaning = "Kh√¥ng ph√°t hi·ªán di chuy·ªÉn ngang."
        report_lines.append(f"{'Lateral Movement':<30} | {lateral_assess:<30} | {lateral_meaning}")
        
        # Row 5: Data Exfiltration
        exfil_assess = f"{exfil_count} transfers nghi ng·ªù"
        if exfil_count > 0:
            exfil_meaning = "C√≥ d·ªØ li·ªáu b·∫•t th∆∞·ªùng ra ngo√†i."
        else:
            exfil_meaning = "Kh√¥ng c√≥ lu·ªìng d·ªØ li·ªáu l·ªõn ra ngo√†i."
        report_lines.append(f"{'Data Exfiltration':<30} | {exfil_assess:<30} | {exfil_meaning}")
        
        # Row 6: ICS detection
        ics_assess = f"{ics_flows} ICS flows, {ics_anomaly_count} anomalies"
        if ics_flows == 0:
            ics_meaning = "PCAP kh√¥ng ph·∫£i m·∫°ng c√¥ng nghi·ªáp."
        elif ics_anomaly_count > 0:
            ics_meaning = "Ph√°t hi·ªán b·∫•t th∆∞·ªùng trong ICS protocol."
        else:
            ics_meaning = "ICS traffic b√¨nh th∆∞·ªùng."
        report_lines.append(f"{'ICS detection':<30} | {ics_assess:<30} | {ics_meaning}")
        
        report_lines.append("")
        
        # === OUTPUT FILES ===
        report_lines.append("3.3 Output Files Sinh Ra")
        report_lines.append("")
        report_lines.append("output/pcap_analysis/")
        report_lines.append("  ‚îú‚îÄ general_anomalies.csv")
        report_lines.append("  ‚îú‚îÄ lateral_movement.csv")
        report_lines.append("  ‚îú‚îÄ data_exfiltration.csv")
        report_lines.append("  ‚îú‚îÄ ics_anomalies.csv")
        report_lines.append("  ‚îî‚îÄ attack_summary.txt (b√°o c√°o n√†y)")
        report_lines.append("")
        report_lines.append("T·∫•t c·∫£ ch·ª©a log chi ti·∫øt: src_ip, dst_ip, port, bytes, score...")
        
        if lateral_count > 0:
            report_lines.append("")
            report_lines.append("ƒê·∫∑c bi·ªát, lateral_movement.csv c√≥ d√≤ng ƒë√°ng ch√∫ √Ω:")
            if lateral_df is not None and len(lateral_df) > 0:
                suspicious = lateral_df[lateral_df['is_lateral_movement'] == 1]
                if len(suspicious) > 0:
                    top = suspicious.iloc[0]
                    unique_dsts = top.get('unique_dsts', top.get('unique_destinations', 0))
                    report_lines.append(f"  src_ip={top.get('src_ip', 'N/A')}  unique_dsts={unique_dsts}  unique_ports={top.get('unique_ports', 0)}  lateral_score={top.get('lateral_score', 0):.1f}")
                    report_lines.append(f"  ‚Üí host n√†y l√† ·ª©ng vi√™n duy nh·∫•t c·∫ßn xem th√™m trong Wireshark.")
        
        report_lines.append("")
        
        # === K·∫æT LU·∫¨N CHUY√äN M√îN ===
        report_lines.append("‚öôÔ∏è K·∫æT LU·∫¨N CHUY√äN M√îN")
        report_lines.append("-" * 110)
        report_lines.append("  ‚Ä¢ Pipeline ho·∫°t ƒë·ªông ƒë√∫ng: file PCAP ƒë∆∞·ª£c parse ‚Üí feature h√≥a ‚Üí model ƒë√°nh gi√° ‚Üí xu·∫•t k·∫øt qu·∫£.")
        
        if total_anomalies == 0 and lateral_count == 0 and exfil_count == 0:
            report_lines.append("  ‚Ä¢ Kh√¥ng ph√°t hi·ªán m·ªëi ƒëe d·ªça r√µ r·ªát. Network traffic trong ph·∫°m vi b√¨nh th∆∞·ªùng.")
        elif total_anomalies > 0 or lateral_count > 0 or exfil_count > 0:
            report_lines.append(f"  ‚Ä¢ Ph√°t hi·ªán {total_anomalies + lateral_count + exfil_count} m·ªëi ƒëe d·ªça ti·ªÅm ·∫©n c·∫ßn ƒëi·ªÅu tra.")
        
        if ics_flows == 0:
            report_lines.append("  ‚Ä¢ ICS traffic = 0 ‚áí ƒë√¢y l√† PCAP IT th√¥ng th∆∞·ªùng, ch∆∞a c√≥ OT/ICS.")
        else:
            report_lines.append(f"  ‚Ä¢ ICS traffic = {ics_flows} flows ‚áí ƒë√¢y l√† m·∫°ng ICS/SCADA/OT.")
        
        report_lines.append("")
        
        # === KHUY·∫æN NGH·ªä ===
        report_lines.append("üí° KHUY·∫æN NGH·ªä")
        report_lines.append("-" * 110)
        
        recommendations = []
        if lateral_count > 0:
            recommendations.append("1. URGENT: ƒêi·ªÅu tra c√°c IP th·ª±c hi·ªán lateral movement")
            recommendations.append("   ‚Üí Xem l·∫°i trong Wireshark, check firewall logs")
            recommendations.append("   ‚Üí Ki·ªÉm tra compromised accounts")
        
        if exfil_count > 0:
            recommendations.append("2. HIGH PRIORITY: Ph√°t hi·ªán data exfiltration")
            recommendations.append("   ‚Üí Block ngay c√°c external IPs ƒë√°ng ng·ªù")
            recommendations.append("   ‚Üí Review data access logs")
        
        if ics_anomaly_count > 0:
            recommendations.append("3. CRITICAL: ICS/SCADA anomalies")
            recommendations.append("   ‚Üí Verify device configurations")
            recommendations.append("   ‚Üí Check unauthorized protocol usage")
        
        if high_risk > 0:
            recommendations.append(f"4. Review {high_risk} high-risk anomalies ƒë∆∞·ª£c model ƒë√°nh d·∫•u")
        
        if not recommendations:
            recommendations.append("‚úÖ Kh√¥ng ph√°t hi·ªán m·ªëi ƒëe d·ªça nghi√™m tr·ªçng.")
            recommendations.append("   ‚Üí Ti·∫øp t·ª•c monitoring th∆∞·ªùng xuy√™n")
            recommendations.append("   ‚Üí C·∫≠p nh·∫≠t baseline model ƒë·ªãnh k·ª≥")
        
        for rec in recommendations:
            report_lines.append(rec)
        
        report_lines.append("")
        report_lines.append("="*110)
        report_lines.append("H·∫æT B√ÅO C√ÅO - End of Analysis Report")
        report_lines.append("="*110)
        
        # Save report
        report_text = "\n".join(report_lines)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        # Also save JSON version
        json_path = output_path.replace('.txt', '.json')
        summary_json = {
            'timestamp': datetime.now().isoformat(),
            'total_flows': int(total_flows),
            'total_anomalies': int(total_anomalies),
            'anomaly_rate': float(anomaly_rate),
            'threats': {
                'lateral_movement': int(lateral_df['is_lateral_movement'].sum()) if lateral_df is not None and 'is_lateral_movement' in lateral_df.columns else 0,
                'data_exfiltration': int(exfil_df['is_exfiltration'].sum()) if exfil_df is not None and 'is_exfiltration' in exfil_df.columns else 0,
                'ics_anomalies': int(len(ics_df)) if ics_df is not None else 0
            },
            'risk_levels': {
                'high': int(high_risk) if 'anomaly_score' in results.columns else 0,
                'medium': int(medium_risk) if 'anomaly_score' in results.columns else 0,
                'low': int(low_risk) if 'anomaly_score' in results.columns else 0
            }
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary_json, f, indent=2)
        
        logger.info(f"‚úÖ Attack summary saved to {output_path}")
        logger.info(f"‚úÖ JSON summary saved to {json_path}")
        
        # Print summary to console
        print("\n" + report_text)
        
        return report_text
